### 1.AudioDVP

直接用ATVGnet网络预训练权重（该权重是从BBC进行大量训练的）将源语音进行提取特征出来，保存pt文件，然后与从视频中提取的几个特征融合来进行训练。

如果要实现多语言话就要对ATVGnet进行重新训练，因为原预训练权重是用大量英语进行训练的。

我的理解，就算我们训练出多语言的ATVGnet后，每次传入新的视频都会把视频拆成图片来进行训练与ATVGnet提取的面部特征融合。再次进行训练。

**是否可以训练多语言ATVGnet，然后训练大量的3D face reconstruction，两个融合后再次训练。最后的测试效果是否好呢 ？**

### 2.SadTalker

ExpNet 学习一个从音频中产生准确表情系数的通用模型非常困难，一是音频到表情不是针对不同身份的一对一映射任务，二是表达式系数中存在一些与音频无关的运动，会影响预测的准确性。

减少这些不确定性。通过第一帧的表情系数β将表情运动与特定的人联系起来。为了减轻自然说话中其他面部组件的运动权重，通过Wav2Lip和深度3D重建的预训练网络，使用仅嘴唇运动系数作为系数目标。

在推理时，如果语音过长，推理时间会随着语音时长变大。长达3分钟左右的最终效果口型基本相似

**注意**：在上传语音的时候要保持语音的绝对干净，只能有人声，如果出现了背景音乐或者噪声，推理结果就会出现把背景音乐或者噪声也加入到其中

### 3.思考

如果用SadTalker的ExpNet来替换AudioDVP里的ATVGnet对语音面部特征的系数，可以替换吗 ?

答：语音面部特征结果替换成功，中文语音效果比原本身好很多，口型基本对上
